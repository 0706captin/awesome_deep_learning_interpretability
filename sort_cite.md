|Year|Publication|Paper|Citation|
|:---:|:---:|:---:|:---:|
|2014|ECCV|Visualizing and Understanding Convolutional Networks|8009|
|2016|KDD|Why should i trust you?: Explaining the predictions of any classifier|2255|
|2014|ICLR|Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps|2014|
|2015|ICLR|Striving for Simplicity: The All Convolutional Net|1762|
|2017|ICCV|Grad-cam: Visual explanations from deep networks via gradient-based localization|1333|
|2015|ICMLW|Understanding Neural Networks Through Deep Visualization|974|
|2016|arxiv|The Mythos of Model Interpretability|951|
|2015|CVPR|Understanding deep image representations by inverting them|929|
|2017|NIPS|A Unified Approach to Interpreting Model Predictions|591|
|2017|ICML|Understanding Black-box Predictions via Influence Functions|517|
|2018|DSP|Methods for interpreting and understanding deep neural networks(scihub)|469|
|2017|CVPR|Knowing when to look: Adaptive attention via a visual sentinel for image captioning|458|
|2017|ICML|Axiomatic attribution for deep networks|448|
|2017|CVPR|Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering|393|
|2017|ICML|Learning Important Features Through Propagating Activation Differences|383|
|2019|AI|Explanation in artificial intelligence: Insights from the social sciences|380|
|2017|CVPR|Network dissection: Quantifying interpretability of deep visual representations|373|
|2019|CSUR|A Survey of Methods for Explaining Black Box Models|344|
|2016|NIPS|Understanding the effective receptive field in deep convolutional neural networks|310|
|2015|AAS|Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model|304|
|2017|ACMSOPP|Deepxplore: Automated whitebox testing of deep learning systems|302|
|2017|ICCV|Interpretable Explanations of Black Boxes by Meaningful Perturbation|284|
|2016|NAACL|Visualizing and understanding neural models in nlp|269|
|2016|CVPR|Inverting Visual Representations with Convolutional Networks|266|
|2018|IJCV|Top-down neural attention by excitation backprop|256|
|2016|NIPS|Synthesizing the preferred inputs for neurons in neural networks via deep generator networks|251|
|2016|EMNLP|Rationalizing Neural Predictions|247|
|2016|ECCV|Generating Visual Explanations|224|
|2016|ICML|Understanding and improving convolutional neural networks via concatenated rectified linear units|216|
|2016|IJCV|Visualizing deep convolutional neural networks using natural pre-images|216|
|2017|ICLR|Visualizing deep neural network decisions: Prediction difference analysis|212|
|2017|arxiv|SmoothGrad: removing noise by adding noise|212|
|2017|arxiv|Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models|210|
|2018|AAAI|Anchors: High-precision model-agnostic explanations|200|
|2016|TVCG|Towards better analysis of deep convolutional neural networks|184|
|2018|ECCV|Deep clustering for unsupervised learning of visual features|167|
|2018|CVPR|Interpretable Convolutional Neural Networks|154|
|2018|FITEE|Visual interpretability for deep learning: a survey|140|
|2016|arxiv|Understanding neural networks through representation erasure|137|
|2018|Access|Peeking inside the black-box: A survey on Explainable Artificial Intelligence (XAI)|131|
|2016|arxiv|Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks|130|
|2017|arxiv|Distilling a neural network into a soft decision tree|126|
|2018|ICLR|Towards better understanding of gradient-based attribution methods for deep neural networks|123|
|2018|NIPS|Sanity Checks for Saliency Maps|122|
|2016|TVCG|Visualizing the hidden activity of artificial neural networks|122|
|2017|TVCG|ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models|113|
|2018|AAAI|Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients|112|
|2017|NIPS|Real time image saliency for black box classifiers|111|
|2018|ICML|Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)|110|
|2017|CVPR|Interpretable 3d human action analysis with temporal convolutional networks|106|
|2017|IJCAI|Right for the right reasons: Training differentiable models by constraining their explanations|102|
|2017|NIPS|SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability|97|
|2019|ExplainAI|The (Un)reliability of saliency methods(scihub)|95|
|2015|ICCV|Understanding deep features with computer-generated imagery|94|
|2018|ICLR|Learning how to explain neural networks: PatternNet and PatternAttribution|90|
|2018|JAIR|Learning Explanatory Rules from Noisy Data|90|
|2016|arxiv|Grad-CAM: Why did you say that?|87|
|2017|CVPR|MDNet: A Semantically and Visually Interpretable Medical Image Diagnosis Network|86|
|2018|WACV|Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks|85|
|2016|CVPR|Visualizing and Understanding Deep Texture Representations|83|
|2016|CVPR|Analyzing Classifiers: Fisher Vectors and Deep Neural Networks|82|
|2018|ICLR|On the importance of single directions for generalization|81|
|2018|CVPR|Tell me where to look: Guided attention inference network|81|
|2017|ICCV|Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention|80|
|2018|CVPR|Multimodal Explanations: Justifying Decisions and Pointing to the Evidence|78|
|2018|arxiv|Manipulating and measuring model interpretability|73|
|2018|ICML|Learning to explain: An information-theoretic perspective on model interpretation|72|
|2017|arxiv|Challenges for transparency|69|
|2017|arxiv|Interpretable & explorable approximations of black box models|68|
|2018|AAAI|Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions|67|
|2017|EMNLP|A causal framework for explaining the predictions of black-box sequence-to-sequence models|64|
|2017|CEURW|What does explainable AI really mean? A new conceptualization of perspectives|64|
|2019|AAAI|Interpretation of neural networks is fragile|63|
|2019|ACL|Attention is not Explanation|57|
|2018|TPAMI|Interpreting deep visual representations via network dissection|56|
|2017|ACL|Visualizing and Understanding Neural Machine Translation|56|
|2019|NMI|Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead|54|
|2019|ACMFAT|Explaining explanations in AI|54|
|2018|CVPR|Transparency by design: Closing the gap between performance and interpretability in visual reasoning|54|
|2018|AAAI|Interpreting CNN Knowledge via an Explanatory Graph|54|
|2018|MIPRO|Explainable artificial intelligence: A survey|54|
|2019|CVPR|Interpreting CNNs via Decision Trees|49|
|2017|survey|Interpretability of deep learning models: a survey of results|49|
|2018|ICML|Extracting Automata from Recurrent Neural Networks Using Queries and Counterexamples|47|
|2019|CVPR|From Recognition to Cognition: Visual Commonsense Reasoning|44|
|2017|arxiv|Towards interpretable deep neural networks by leveraging adversarial examples|44|
|2017|CVPR|Improving Interpretability of Deep Neural Networks with Semantic Information|43|
|2016|arxiv|Attentive Explanations: Justifying Decisions and Pointing to the Evidence|41|
|2018|ECCV|Explainable neural computation via stack neural module networks|40|
|2018|CVPR|Net2vec: Quantifying and explaining how concepts are encoded by filters in deep neural networks|39|
|2017|ICCV|Understanding and comparing deep neural networks for age and gender classification|39|
|2018|ECCV|Grounding visual explanations|38|
|2019|NIPS|This looks like that: deep learning for interpretable image recognition|35|
|2018|NIPS|Explanations based on the missing: Towards contrastive explanations with pertinent negatives|35|
|2017|IJCAI|Understanding and improving convolutional neural networks via concatenated rectified linear units|35|
|2018|ACL|Did the Model Understand the Question?|34|
|2018|ICLR|Detecting statistical interactions from neural network weights|30|
|2018|ECCV|Textual explanations for self-driving vehicles|30|
|2018|BMVC|Rise: Randomized input sampling for explanation of black-box models|30|
|2017|arxiv|Contextual Explanation Networks|28|
|2016|ICML|Visualizing and comparing AlexNet and VGG using deconvolutional layers|28|
|2018|NIPS|Towards robust interpretability with self-explaining neural networks|27|
|2018|AIES|Detecting Bias in Black-Box Models Using Transparent Model Distillation|27|
|2018|arxiv|How convolutional neural network see the world-A survey of convolutional neural network visualization methods|27|
|2018|ECCV|Interpretable basis decomposition for visual explanation|26|
|2018|NIPS|Attacks meet interpretability: Attribute-steered detection of adversarial samples|26|
|2017|ICLR|Exploring LOTS in Deep Neural Networks|26|
|2017|AAAI|Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning|26|
|2018|arxiv|Revisiting the importance of individual units in cnns via ablation|25|
|2018|AAAI|Examining CNN Representations with respect to Dataset Bias|24|
|2016|arxiv|Investigating the influence of noise and distractors on the interpretation of neural networks|24|
|2016|IJCV|Visualizing Object Detection Features|22|
|2018|ICLR|Interpretable counting for visual question answering|21|
|2018|CVPR|What have we learned from deep representations for action recognition?|20|
|2018|CVPR|Learning to Act Properly: Predicting and Explaining Affordances from Images|17|
|2018|ECCV|Convnets and imagenet beyond accuracy: Understanding mistakes and uncovering biases|17|
|2018|NIPS Workshop|Interpretable Convolutional Filters with SincNet|17|
|2019|JVCIR|Interpretable convolutional neural networks via feedforward design|16|
|2019|ICLR|Hierarchical interpretations for neural network predictions|15|
|2018|NIPS|DeepPINK: reproducible feature selection in deep neural networks|15|
|2017|CVPR|Mining Object Parts from CNNs via Active Question-Answering|15|
|2019|CVPR|Attention branch network: Learning of attention mechanism for visual explanation|14|
|2017|CVPRW|Looking under the hood: Deep neural network visualization to interpret whole-slide image analysis outcomes for colorectal polyps|14|
|2018|CVPR|Teaching Categories to Human Learners with Visual Explanations|13|
|2018|ECCV|Vqa-e: Explaining, elaborating, and enhancing your answers for visual questions|12|
|2018|NIPS|Representer point selection for explaining deep neural networks|11|
|2016|ECCV|Design of kernels in convolutional neural networks for image classification|11|
|2019|ICLR|How Important Is a Neuron?|10|
|2017|ICCV|Learning to disambiguate by asking discriminative questions|10|
|2019|AAAIW|Unsupervised Learning of Neural Networks to Explain Neural Networks|9|
|2018|CVPR|What do Deep Networks Like to See?|9|
|2019|CVPR|Interpretable and fine-grained visual explanations for convolutional neural networks|8|
|2018|ECCV|Choose Your Neuron: Incorporating Domain Knowledge through Neuron-Importance|8|
|2019|ICLR|Visual Explanation by Interpretation: Improving Visual Feedback Capabilities of Deep Neural Networks|7|
|2019|ICAIS|Interpreting black box predictions using fisher kernels|7|
|2019|CVPR|Learning to Explain with Complemental Examples|6|
|2019|ICCV|U-CAM: Visual Explanation using Uncertainty based Class Activation Maps|6|
|2019|ICCV|Towards Interpretable Face Recognition|6|
|2019|CVPR|Revealing Scenes by Inverting Structure from Motion Reconstructions|5|
|2019|ICCV|Taking a HINT: Leveraging Explanations to Make Vision and Language Models More Grounded|5|
|2018|CVPR|Interpret Neural Networks by Identifying Critical Data Routing Paths|5|
|2018|ECCV|Diverse feature visualizations reveal invariances in early layers of deep neural networks|5|
|2019|ICML|Towards A Deep and Unified Understanding of Deep Neural Models in NLP|4|
|2019|AAAI|Classifier-agnostic saliency map extraction|4|
|2019|AAAIW|Network Transplanting|4|
|2019|arxiv|Attention Interpretability Across NLP Tasks|4|
|2019|NIPS|A benchmark for interpretability methods in deep neural networks（同arxiv:1806.10758）|3|
|2019|arxiv|Interpretable CNNs|3|
|2019|NIPS|Full-gradient representation for neural network visualization|2|
|2019|NIPS|On the (In) fidelity and Sensitivity of Explanations|2|
|2019|ICCV|Understanding Deep Networks via Extremal Perturbations and Smooth Masks|2|
|2019|NIPS|Towards Automatic Concept-based Explanations|1|
|2019|NIPS|CXPlain: Causal explanations for model interpretation under uncertainty|1|
|2019|CVPR|Multimodal Explanations by Predicting Counterfactuality in Videos|1|
|2019|CVPR|Visualizing the Resilience of Deep Convolutional Network Interpretations|1|
|2019|ICCV|Explaining Neural Networks Semantically and Quantitatively|1|
|2018|arxiv|Computationally Efficient Measures of Internal Neuron Importance|1|
|2020|ICLR|Knowledge Isomorphism between Neural Networks|0|
|2020|ICLR|Interpretable Complex-Valued Neural Networks for Privacy Protection|0|
|2019|AAAI|Can You Explain That? Lucid Explanations Help Human-AI Collaborative Image Retrieval|0|
|2018|ECCV|ExplainGAN: Model Explanation via Decision Boundary Crossing Transformations|0|
